# ?? Server ???????????????????????????????????????????????????????????????????
quarkus.http.port=8082
quarkus.application.name=llm-chat-ui

# CORS - allow all for local dev
quarkus.http.cors=true
quarkus.http.cors.origins=*

# ?? LLM server connection ?????????????????????????????????????????????????????
#
# Point this at your local LLM server.
# Common values:
#   llama.cpp / LM Studio (OpenAI mode) : http://localhost:8080
#   Ollama                               : http://localhost:11434
#   Jan                                  : http://localhost:1337
#   LM Studio (default port)             : http://localhost:1234
#
llm.api.url=http://localhost:8081
llm.api.model=local-model
llm.api.max-tokens=2048
llm.api.temperature=0.7

# ?? API endpoint paths ????????????????????????????????????????????????????????
#
# Standard OpenAI-compatible path (llama.cpp, LM Studio, Jan, vLLM ?)
llm.api.path.completions=v1/chat/completions
llm.api.path.models=v1/models
#
# Ollama uses the same OpenAI-compatible paths since v0.1.24:
#   llm.api.path.completions=v1/chat/completions
#   llm.api.path.models=v1/models
#
# If your server uses a non-standard path, override here, e.g.:
#   llm.api.path.completions=api/chat/completions
#   llm.api.path.models=api/tags

# ?? REST client timeouts ??????????????????????????????????????????????????????
quarkus.rest-client."com.llmchat.service.LlmApiClient".url=${llm.api.url}
quarkus.rest-client."com.llmchat.service.LlmApiClient".connect-timeout=10000
# Large models can be slow ? 120 s default, increase for slow hardware
quarkus.rest-client."com.llmchat.service.LlmApiClient".read-timeout=120000

# ?? Logging ???????????????????????????????????????????????????????????????????
quarkus.log.level=INFO
quarkus.log.category."com.llmchat".level=DEBUG

# ?? Static resources ??????????????????????????????????????????????????????????
quarkus.http.static-resources.index-page=index.html

quarkus.package.jar.type=uber-jar